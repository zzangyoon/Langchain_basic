{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d709dd7",
   "metadata": {},
   "source": [
    "# 1. 벡터 저장소가 이미 있는 상황"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180916c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv      # api-key\n",
    "from langchain_chroma import Chroma # DB\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd24753",
   "metadata": {},
   "source": [
    "## 1. 벡터 스토어 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040f1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\"\n",
    ")\n",
    "persist_directory = \"../vectorStore/samsung_2025_db\"\n",
    "collection_name = \"samsung2025\"\n",
    "vectorStore = Chroma(\n",
    "    persist_directory = persist_directory,\n",
    "    collection_name = collection_name,\n",
    "    embedding_function = embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cccc959",
   "metadata": {},
   "source": [
    "## 2. Retriever 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4858f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorStore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\" : 30}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29697a",
   "metadata": {},
   "source": [
    "## 3. Reranker 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95fcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cross_encoders.huggingface import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "hf_ce = HuggingFaceCrossEncoder(\n",
    "    model_name = \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "    model_kwargs = {\n",
    "        \"device\" : \"cuda\",\n",
    "        \"max_length\" : 512\n",
    "    }\n",
    ")\n",
    "\n",
    "reranker = CrossEncoderReranker(\n",
    "    model = hf_ce,\n",
    "    top_n = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa655ba4",
   "metadata": {},
   "source": [
    "## 4. Retriever -> Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8819b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "comp_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever = retriever,\n",
    "    base_compressor = reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a7a1f",
   "metadata": {},
   "source": [
    "## 5. Reorder (순서 정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0281ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "reorder = LongContextReorder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c68223",
   "metadata": {},
   "source": [
    "## 6. 검색결과 문서 합치는 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da53a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    result = []\n",
    "    for item in docs:\n",
    "        result.append(item.page_content)\n",
    "    return \"\\n\\n---\\n\\n\".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae2111",
   "metadata": {},
   "source": [
    "# 2. 기본 체인 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b45fe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라.\\n\\n     [컨텍스트]\\n     {context}\\n\\n    '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000018F97DBF450>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018F97D9CBD0>, root_client=<openai.OpenAI object at 0x0000018F97D9F150>, root_async_client=<openai.AsyncOpenAI object at 0x0000018F97DBE490>, model_name='gpt-4.1-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 프롬프트 설정\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라.\n",
    "     \n",
    "     [컨텍스트]\n",
    "     {context}\n",
    "\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 2. 모델 설정\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4.1-mini\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 3. outputparser\n",
    "outputparser = StrOutputParser()\n",
    "\n",
    "# 4. 체인 설정\n",
    "chain = rag_prompt | model | outputparser\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a4cae",
   "metadata": {},
   "source": [
    "# 3. 통합 체인 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5517df21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"삼성전자는 인재와 기술을 바탕으로 최고의 제품과 서비스를 창출하여 인류사회에 공헌한다는 경영철학 아래 기술 리더십으로 재도약의 기반을 다지고, 새로운 영역에서 미래 성장동력을 확보해 나갈 계획입니다. 또한, 2025년에는 '삼성 청년SW·AI아카데미' 교육 기회를 마이스터고 졸업생까지 확대하고, '삼성 희망디딤돌' 인천센터를 추가 설립하여 더 많은 청년을 지원할 예정입니다.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"docs\" : RunnableLambda(lambda x: comp_retriever.invoke(x[\"question\"])),\n",
    "        \"question\" : RunnablePassthrough()\n",
    "    }\n",
    "    | RunnableLambda(lambda x : {\n",
    "        \"context\" : format_docs(reorder.transform_documents(x[\"docs\"])),\n",
    "        \"question\" : x[\"question\"]\n",
    "    })\n",
    "    | chain\n",
    ")\n",
    "\n",
    "rag_chain.invoke({\n",
    "    \"question\" : \"삼성의 미래 계획은 어떻게 되나요?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7950775a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  docs: RunnableLambda(...),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| RunnableLambda(...)\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라.\\n\\n     [컨텍스트]\\n     {context}\\n\\n    '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000018F97DBF450>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018F97D9CBD0>, root_client=<openai.OpenAI object at 0x0000018F97D9F150>, root_async_client=<openai.AsyncOpenAI object at 0x0000018F97DBE490>, model_name='gpt-4.1-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a123e",
   "metadata": {},
   "source": [
    "# 4. multi_input_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71a38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_retriever_chain = RunnableLambda(lambda x: x[\"question\"]) | comp_retriever | format_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e7c105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성전자는 인재와 기술을 바탕으로 최고의 제품과 서비스를 창출하여 인류사회에 공헌한다는 경영철학 아래, 기술 리더십으로 재도약의 기반을 다지고 새로운 영역에서 미래 성장동력을 확보해 나갈 계획입니다. 또한, 지속가능한 성장 기반 마련을 위해 이해관계자의 의견에 귀 기울이며 지속적으로 노력할 예정입니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\" : RunnableLambda(lambda x: x[\"question\"]) | comp_retriever | format_docs,\n",
    "        \"question\" : RunnableLambda(lambda x: x[\"question\"])\n",
    "    }\n",
    "    | chain\n",
    ")\n",
    "rag_chain.invoke({\n",
    "    \"question\" : \"삼성의 미래 계획은 어떻게 되나요?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d660c9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'pro', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라.\\n\\n     [컨텍스트]\\n     {context}\\n\\n    '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['pro', 'question'], input_types={}, partial_variables={}, template='{pro}에 맞게 {question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000018F97DC2D10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018F5F4E8510>, root_client=<openai.OpenAI object at 0x0000018F99B77150>, root_async_client=<openai.AsyncOpenAI object at 0x0000018F989AEC50>, model_name='gpt-4.1-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 프롬프트 설정\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라.\n",
    "     \n",
    "     [컨텍스트]\n",
    "     {context}\n",
    "\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{pro}에 맞게 {question}\")\n",
    "])\n",
    "\n",
    "# 2. 모델 설정\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4.1-mini\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# 3. outputparser\n",
    "outputparser = StrOutputParser()\n",
    "\n",
    "# 4. 체인 설정\n",
    "chain = rag_prompt | model | outputparser\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186ee201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성은 냥냥, 미래에도 인재와 기술로 최고 제품과 서비스 만들면서 인류사회에 공헌할 계획이냥! 지속가능경영으로 재도약하고, 새로운 영역에서 성장동력 확보할 거라냥!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\" : com_retriever_chain,\n",
    "        \"question\" : RunnableLambda(lambda x: x[\"question\"]),\n",
    "        \"pro\" : RunnableLambda(lambda x: x[\"pro\"])\n",
    "    }\n",
    "    | chain\n",
    ")\n",
    "rag_chain.invoke({\n",
    "    \"question\" : \"삼성의 미래 계획은 어떻게 되나요?\",\n",
    "    \"pro\" : \"냥냥체\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
